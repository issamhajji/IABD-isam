{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plistlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we are using \"Veggies\" dataset from the getgo\n",
    "dataset = \"../recursos/datasets/1FA/veggies-keras/train/\"\n",
    "dataset_dir = pathlib.Path(dataset)\n",
    "print(dataset)\n",
    "\n",
    "image_count = len(list(dataset_dir.glob('*.jpg')))\n",
    "\n",
    "print(image_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we import the classnames\n",
    "\n",
    "df = pd.read_csv(\"../recursos/datasets/1FA/veggies-keras/_classes.csv\")\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "class_names = df.columns[1:].str.strip().tolist()\n",
    "\n",
    "df = df.iloc[:2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SIZE = 200\n",
    "X_dataset = []\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    img = image.load_img(dataset +df['filename'][i], target_size=(SIZE,SIZE,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    X_dataset.append(img)\n",
    "\n",
    "\n",
    "X = np.array(X_dataset)\n",
    "\n",
    "print(\"result:\")\n",
    "print(df['filename'][200])\n",
    "\n",
    "\n",
    "y = np.array(df.drop(['filename'], axis=1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we work the dataset now, we set de batch size and the image size\n",
    "\n",
    "batch_size = 64\n",
    "img_height, img_width = 180, 180\n",
    "\n",
    "# we already divided the dataset into train, val and test so we don't need to specify the subset and split parameters\n",
    "\n",
    "# train dataset\n",
    "# train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     dataset_dir,\n",
    "#     validation_split = 0.2,\n",
    "#     subset = \"training\",\n",
    "#     seed=123,\n",
    "#     image_size=(img_height,img_width),\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "\n",
    "# # val dataset\n",
    "# val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#     dataset_dir,\n",
    "#     validation_split = 0.2,\n",
    "#     subset = \"validation\",\n",
    "#     seed=123,\n",
    "#     image_size=(img_height,img_width),\n",
    "#     batch_size=batch_size\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class_names = train_ds.class_names\n",
    "# print(class_names)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Input(shape = (SIZE, SIZE, 3)),\n",
    "  # normalization here\n",
    "  layers.Rescaling(1./255),\n",
    "  layers.Conv2D(8, 3, padding = 'same', activation = 'relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(8, 3, padding = 'same', activation = 'relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(4, 3, padding = 'same', activation = 'relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation = 'relu'),\n",
    "  layers.Dense(len(class_names), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs=10\n",
    "history = model.fit(\n",
    "  X_train, \n",
    "  y_train,\n",
    "  validation_data = (X_test, y_test),\n",
    "  epochs = epochs,\n",
    "  batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "history = history.history\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 8))\n",
    "# train/validation loss\n",
    "axs[0].plot(range(epochs), history['loss'], label = 'Training Loss')\n",
    "axs[0].plot(range(epochs), history['val_loss'], label = 'Validation Loss')\n",
    "axs[0].legend(loc = 'upper right')\n",
    "axs[0].set_title('Training and Validation Loss')\n",
    "# train/validation accuracy\n",
    "axs[1].plot(range(epochs), history['accuracy'], label = 'Training Accuracy')\n",
    "axs[1].plot(range(epochs), history['val_accuracy'], label = 'Validation Accuracy')\n",
    "axs[1].legend(loc = 'lower right')\n",
    "axs[1].set_title('Training and Validation Accuracy')\n",
    "#\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### LET'S PREDICT\n",
    "\n",
    "# image preprocessing\n",
    "img_url = image.load_img(\"../recursos/fotos_ingredientes/veggie-tops-1.jpg\", target_size=(SIZE,SIZE))\n",
    "img_arr = image.img_to_array(img_url)\n",
    "img_arr = img_arr/255\n",
    "img_arr = np.expand_dims(img_arr,axis=0)\n",
    "\n",
    "pred = model.predict(img_arr)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# results\n",
    "\n",
    "# for i, p in enumerate(pred):\n",
    "#     print(f\"{class_names[i]}: {p:.2f}\")\n",
    "\n",
    "threshold=0.00\n",
    "top_preds = [(class_names[i], pred[i]) for i in range(len(pred)) if pred[i] > threshold]\n",
    "\n",
    "# Show image\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img_url)\n",
    "plt.axis('off')\n",
    "\n",
    "# Add predicted labels\n",
    "label_text = \"\\n\".join([f\"{cls}: {score:.2f}\" for cls, score in top_preds])\n",
    "plt.gcf().text(\n",
    "        0.02, 0.98, label_text,\n",
    "        fontsize=12, va='top', ha='left',\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "plt.title(\"Predicted Classes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
